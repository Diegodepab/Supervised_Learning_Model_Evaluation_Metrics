# Supervised Learning Model Evaluation Metrics

This repository focuses on evaluating the performance of supervised learning models through various metrics derived from the confusion matrix. It includes analysis and comparisons of methods applied to hypothetical datasets, showcasing the complexities in assessing model performance.

## Contents

- **Data Analysis**: Examination of an unbalanced dataset with 1000 samples across two classes.
- **Evaluation Metrics**:
  - Precision (PR)
  - Recall (RC)
  - Specificity (SP)
  - False Negative Rate (FNR)
  - False Positive Rate (FPR)
  - Accuracy (ACC)
  - Jaccard Index (J)
  - F-measure (Fm)
  
- **Visualizations**: Graphical representations including heatmaps and radar plots for metric comparisons.

## Dataset

The dataset consists of 1000 samples categorized into two classes: true and false. The dataset is unbalanced, with 100 true instances and 900 false instances, which can lead to biased evaluations and misleading results. 



Initially this repository is to reuse information to define each metric well, in addition to carrying out a task for the university.
